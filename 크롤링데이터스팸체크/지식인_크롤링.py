# -*- coding: utf-8 -*-
"""지식인 크롤링.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S3DNDg8nLhmq0AURKv8VIsukchne2HqV
"""

!pip install beautifulsoup4 requests
!pip install chromedriver_autoinstaller
!pip install konlpy wordcloud
!pip install selenium
!pip install webdriver_manager





import requests
from lxml import html

# 크롤링할 페이지 수
num_pages = 3
base_url = 'https://kin.naver.com/qna/list.naver?dirId=203'
url = base_url

for page in range(1, num_pages + 1):
    # 웹 페이지 요청
    response = requests.get(url)

    # HTML 파싱
    tree = html.fromstring(response.text)
    # 페이지에서 데이터 수집
    xpath = '/html/body/div[2]/div[3]/div/div/div[1]/div[3]/div/table/tbody/tr/td[1]/a'
    results = tree.xpath(xpath)

    # 페이지별로 수집한 데이터 출력
    print(f"페이지 {page} 데이터:")
    for idx, result in enumerate(results, start=1):
        print(f"요소 {idx}의 텍스트: {result.text}")

    # 다음 페이지로 이동하기 위해 페이지 번호를 증가시킴
    url = f'{base_url}&page={page + 1}'
#csv변환 기능만 없는 버전

import requests
from lxml import html
import pandas as pd

# 크롤링할 페이지 수
num_pages = 3
base_url = 'https://kin.naver.com/qna/list.naver?dirId=203'
url = base_url

data = []

for page in range(1, num_pages + 1):
    # 웹 페이지 요청
    response = requests.get(url)

    # HTML 파싱
    tree = html.fromstring(response.text)

    # 페이지에서 데이터 수집
    xpath = '/html/body/div[2]/div[3]/div/div/div[1]/div[3]/div/table/tbody/tr/td[1]/a'
    results = tree.xpath(xpath)

    # 데이터 추출 및 라벨링
    for idx, result in enumerate(results, start=1):
        data.append({
            '페이지': page,
            '요소 번호': idx,
            '텍스트': result.text
        })

    # 다음 페이지로 이동하기 위해 페이지 번호를 증가시킴
    url = f'{base_url}&page={page + 1}'

# 데이터를 Pandas DataFrame으로 변환
df = pd.DataFrame(data)

# DataFrame을 CSV 파일로 저장
df.to_csv('크롤링_데이터.csv', index=False, encoding='utf-8-sig')

print("데이터를 CSV 파일로 저장했습니다.")

import requests
from lxml import html
import pandas as pd
import re  # 정규 표현식을 사용하기 위한 모듈 임포트

# 크롤링할 페이지 수
num_pages = 3
base_url = 'https://kin.naver.com/qna/list.naver?dirId=203'
url = base_url

data = []

# 정규 표현식 패턴 설정
pattern = re.compile(r'발로란트|배그', re.IGNORECASE)

for page in range(1, num_pages + 1):
    # 웹 페이지 요청
    response = requests.get(url)

    # HTML 파싱
    tree = html.fromstring(response.text)

    # 페이지에서 데이터 수집
    xpath = '/html/body/div[2]/div[3]/div/div/div[1]/div[3]/div/table/tbody/tr/td[1]/a'
    results = tree.xpath(xpath)

    # 데이터 추출 및 라벨링
    for idx, result in enumerate(results, start=1):
        text = result.text
        label = 0 if pattern.search(text) else 1
        data.append({
            '페이지': page,
            '요소 번호': idx,
            '텍스트': text,
            '라벨': label
        })

    # 다음 페이지로 이동하기 위해 페이지 번호를 증가시킴
    url = f'{base_url}&page={page + 1}'

# 데이터를 Pandas DataFrame으로 변환
df = pd.DataFrame(data)

# DataFrame을 CSV 파일로 저장
df.to_csv('크롤링_데이터(ver.2).csv', index=False, encoding='utf-8-sig')

print("데이터를 CSV 파일로 저장했습니다.")

